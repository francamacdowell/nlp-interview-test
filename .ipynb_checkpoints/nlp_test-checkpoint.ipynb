{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5JWXZ-E26KOf"
   },
   "source": [
    "# NLP Engineer Internship Test\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Make an intent classifier with the dataset provided in the repository in which the imput will be a phrase and the model must correctly classify it in one of the intentions defined in the dataset\n",
    "- The main objective of the test is to assess the candidate's ability to solve problems and demonstrate his attempts, hypotheses, solutions clearly and the ability to analyze data, extract information, suggest conclusions, and support decision-making.\n",
    "\n",
    "### Example interaction with the resulting model\n",
    "Just a demonstrative example to understand the problem, the output can be formatted according to the candidate's preference so that it shows the intention and the confidence that the model returned\n",
    "```\n",
    "example 1:\n",
    "input: como ocorre a transferência de processos para o SEI?\n",
    "output: \n",
    "intent: processos_administrativos \n",
    "confidence: 96.7%\n",
    "\n",
    "example 2:\n",
    "input: quais são os documentos disponíveis?\n",
    "output: \n",
    "intent: documentação\n",
    "confidence: 83.2%\n",
    "```\n",
    "\n",
    "## Information about the datasets\n",
    "\n",
    "The dataset is in the dataset.txt file in the repository, it is in the format:\n",
    "\n",
    "```\n",
    "intent:intent_name1\n",
    "-sentence1\n",
    "-sentence2\n",
    "...\n",
    "-sentenceN\n",
    "\n",
    "intent:intent_name2\n",
    "-sentence1\n",
    "-sentence2\n",
    "...\n",
    "-sentenceN\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "- Put all the code, logic, hypotheses, etc. organized in the nlp_test.pynb jupyter notebook of the repository (i.e. specific actions you would take based on your analysis) and methodology.\n",
    "- Fork this repository\n",
    "- Make your fork private\n",
    "- Give access to the user \"lucasraggi89\" in your private fork\n",
    "- Make all changes in your fork\n",
    "- Only the commits before the deadline will be considered\n",
    "\n",
    "Tip: it is interesting to put your different approaches and attempts in the notebook to support decision making<br/>\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gn0sWRCSx89T"
   },
   "source": [
    "___\n",
    "# Starting my test here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction to the problem](#Introduction-to-the-problem)\n",
    "- [The dataset](#The-dataset)\n",
    "- [Preprocessing our data](#Preprocessing-our-data)\n",
    "- [Feature Extraction: Transforming words in numbers](#Feature-Extraction:-Transforming-words-in-numbers)\n",
    "- [Taking a look at the ML models (Model Selection)](#Taking-a-look-at-the-ML-models-(Model-Selection))\n",
    "- [Conclusion and Decision](#Conclusion-and-Decision)\n",
    "- [Interaction with the resulting model](#Interaction-with-the-resulting-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given problem is a NLP problem, where we receive an user's interaction phrase, and the task is to process this content and classify in useful **intents** to make smartful automated decisions, generally used in chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided data is simple, consists in one *.txt* file with some inputs separeted by the intent class. As it is in no structured format (as *.json*, *.csv*, etc), I'm going to create a parser function which transforms this text data into python *dict*, to facility the data wrangling, manipulations and Pandas DataFrame creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def txt_data_to_dict(filepath):\n",
    "    # creating dict which\n",
    "    data_dict = {\n",
    "                'phrase': [],\n",
    "                'intent': [],\n",
    "                }\n",
    "    try:\n",
    "        with open(filepath, 'r') as txt_file:\n",
    "            # processing the lines\n",
    "            intent_class = None\n",
    "            phrase = None\n",
    "            for line in txt_file:\n",
    "                if 'intent' in line:\n",
    "                    # get intent class and remove '\\n' charactere\n",
    "                    intent_class = line.split(':')[1].replace('\\n', '')\n",
    "                    continue\n",
    "                if '-' in line:\n",
    "                    # get string after '-' and remove blank spaces from the end and beginning\n",
    "                    phrase = line[1:].strip()\n",
    "                    # adding this 'row' to the dict\n",
    "                    data_dict['phrase'].append(phrase)\n",
    "                    data_dict['intent'].append(intent_class)\n",
    "            \n",
    "            return data_dict\n",
    "    except FileNotFoundError:\n",
    "        print('The file was not found, make sure the filepath is correct.')\n",
    "            \n",
    "            \n",
    "path = 'data.txt'\n",
    "data = txt_data_to_dict(path)\n",
    "# Creating pandas DataFrame from dict:\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here we have a pandas DataFrame with two columns:\n",
    "1. The user input **phrase**\n",
    "2. The class which represents the **intent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apg</td>\n",
       "      <td>competencias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APG</td>\n",
       "      <td>competencias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assessoria de Planejamento e Gestão</td>\n",
       "      <td>competencias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onde a  diretoria financeira  atua</td>\n",
       "      <td>competencias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qual a função principal da  diretoria geral ?</td>\n",
       "      <td>competencias</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          phrase        intent\n",
       "0                                            apg  competencias\n",
       "1                                            APG  competencias\n",
       "2            Assessoria de Planejamento e Gestão  competencias\n",
       "3             onde a  diretoria financeira  atua  competencias\n",
       "4  qual a função principal da  diretoria geral ?  competencias"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also important take a look at the class distribution in our dataset, because it must impact directly at the result model. So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count_classes = pd.value_counts(df['intent'], sort = True) / len(df['intent'])\n",
    "\n",
    "# Creating a plot with bar kind:\n",
    "count_classes.plot(kind = 'bar', rot=0, figsize=(9, 7))\n",
    "\n",
    "# Setting plotting title and axi's legends:\n",
    "_ = plt.title(\"Data Class distribution\")\n",
    "_ = plt.xlabel(\"Class\")\n",
    "_ = plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider that the dataset is balanced, it's not critical enough to use imbalanced data techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset ready, we start the most important phase in a Data Science pipeline and must start to apply some preprocessing NLP techniques to get a better representation of the data and input to the ML model. As the saying goes \"Garbage in, garbage out!\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converted Accented Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words with accent marks like “não” and “usuário” can be converted and standardized to “nao” and “usuario”, else our NLP model will treat “não” and “nao” as different words even though they are referring to same thing. To do this, we use the module unidecode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If unidecode library isn't installed, uncomment the line bellow to install\n",
    "# !pip install unidecode\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "def remove_accented_chars(dict_data, column_name):\n",
    "    \"\"\"remove accented characters from text, e.g. usuário\"\"\"\n",
    "    # Create lambda function to decode\n",
    "    remove_accent = lambda x: unidecode(x)\n",
    "    # Apply lambda with map() in list\n",
    "    no_accented_phrases = list(map(remove_accent, dict_data[column_name]))\n",
    "    # Get new values without accent\n",
    "    dict_data[column_name] = no_accented_phrases\n",
    "    \n",
    "    return dict_data\n",
    "\n",
    "data = remove_accented_chars(data, 'phrase')\n",
    "# Creating pandas DataFrame from dict:\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing ALL your text data is one of the simplest and most effective form of text preprocessing. It is applicable to most text mining and NLP problems and can help in cases where your dataset is not very large and significantly helps with consistency of expected output decreasing the variation of words to be understood by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing_data(dict_data, column_name):\n",
    "    \"\"\"remove accented characters from text, e.g. usuário\"\"\"\n",
    "    # Create lambda function to lowercase\n",
    "    lowercasing = lambda x: x.lower()\n",
    "    # Apply lambda with map() in list\n",
    "    lowercased_phrases = list(map(lowercasing, dict_data[column_name]))\n",
    "    # Get new lowercased values\n",
    "    dict_data[column_name] = lowercased_phrases\n",
    "    \n",
    "    return dict_data\n",
    "\n",
    "data = lowercasing_data(data, 'phrase')\n",
    "# Creating pandas DataFrame from dict:\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Blank Space Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(data, column_name):\n",
    "    # Create lambda function to remove extra blank spaces from input\n",
    "    remove_spaces = lambda x: \" \".join(x.split())\n",
    "    # Apply lambda with map() in a list\n",
    "    cleaned_phrases = list(map(remove_spaces, data[column_name]))\n",
    "    # Get new lowercased values\n",
    "    data[column_name] = cleaned_phrases\n",
    "    \n",
    "    return data\n",
    "        \n",
    "data = remove_extra_spaces(data, 'phrase')\n",
    "# Creating pandas DataFrame from python dict:\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are a set of commonly used words in a language. Examples of stop words in Portuguese are “a”, “o”, “é”, “da” and etc. The intuition behind using stop words is that, by removing low information words from text, we can focus on the **important** words instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If spacy library isn't installed, uncomment the line bellow to install\n",
    "# !pip install spacy\n",
    "\n",
    "from spacy.lang.pt import Portuguese\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    # Load Portuguese tokenizer, tagger, parser, NER and word vectors\n",
    "    nlp = Portuguese()\n",
    "    size = len(data['phrase'])\n",
    "    # New dict to put preprocessed data\n",
    "    cleaned_dict = {\n",
    "                'phrase': [],\n",
    "                'intent': [],\n",
    "                }\n",
    "    \n",
    "    for i in range(size):\n",
    "        my_doc = nlp(data['phrase'][i])\n",
    "        intent_class = data['intent'][i]\n",
    "        \n",
    "        # Create list of word tokens\n",
    "        token_list = []\n",
    "        \n",
    "        for token in my_doc:\n",
    "            token_list.append(token.text)\n",
    "        \n",
    "        # Create list of word tokens after removing stopwords\n",
    "        filtered_sentence = ''\n",
    "\n",
    "        for word in token_list:\n",
    "            lexeme = nlp.vocab[word]\n",
    "            if lexeme.is_stop == False:\n",
    "                filtered_sentence += word + ' '\n",
    "                \n",
    "        cleaned_dict['phrase'].append(filtered_sentence[:-1])\n",
    "        cleaned_dict['intent'].append(intent_class)\n",
    "        filtered_sentence = ''\n",
    "    \n",
    "    return cleaned_dict\n",
    "\n",
    "data = remove_stop_words(data)\n",
    "# Creating pandas DataFrame from dict:\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Normalization is a process of transforming a word into a single canonical form. This can be done by two processes, stemming and lemmatization. I'm going to choose **lemmatization** because I'm using SpaCy and has no module for stemming. To perform lemmatization, check out the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "\n",
    "def normalize_data(data):\n",
    "    nlp = Portuguese()\n",
    "    size = len(data['phrase'])\n",
    "    \n",
    "    normalized_dict = {\n",
    "                'phrase': [],\n",
    "                'intent': [],\n",
    "                }\n",
    "    for i in range(size):\n",
    "        doc = nlp(data['phrase'][i])\n",
    "        intent_class = data['intent'][i]\n",
    "        lemma_word = ''\n",
    "        for token in doc:\n",
    "            lemma_word += token.lemma_ + ' '\n",
    "        normalized_dict['phrase'].append(lemma_word[:-1])\n",
    "        normalized_dict['intent'].append(intent_class)\n",
    "        lemma_word = ''\n",
    "        \n",
    "    return normalized_dict\n",
    "\n",
    "data = normalize_data(data)\n",
    "# Creating pandas DataFrame from dict:\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More treatments that I won't use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exist more strategies to preprocess the data to the NLP problems, but I judge them not necessary for our dataset. I going to list a few bellow:\n",
    "\n",
    "- Convert number words to numeric form -> Convert \"três\" to \"3\"\n",
    "- Remove numbers -> Identify number like \"3\" and remove them\n",
    "- Remove pontuation -> Remove from phrase characteres like: '!', '?', ','\n",
    "- Text Enrichment / Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Transforming words in numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run machine learning algorithms (which are mathematical/statistical models) we need to convert the text into numbers. For this, we have some famous approches like:\n",
    "\n",
    "- Count Vector\n",
    "- TF-IDF Count\n",
    "- Word2Vec\n",
    "- Doc2Vec\n",
    "\n",
    "I'm going to use some of them to transform our dataset and pass to the next step of pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "tfid_vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorizer_methods = [count_vectorizer, tfid_vectorizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a look at the ML models (Model Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['phrase']\n",
    "y = df['intent']\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach at model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to choose models from different categories, e.g. linear models, ensemble methods, SVMs..\n",
    "\n",
    "And evaluate each model tuning the hyper-parameters with a Grid Search and Cross Validation.\n",
    "\n",
    "For this, I'll build a general function which receives the classifier, parameters for hyper-tuning, vectorizer methods and the metrics to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "       \n",
    "def search_best_model(classifier, scores, tuned_parameters, vectorizer_methods):\n",
    "\n",
    "    for score in scores:\n",
    "        for vect in vectorizer_methods:\n",
    "            print(\"# Tuning hyper-parameters with \" + vect.__class__.__name__ + \" for %s\" % score)\n",
    "            print()\n",
    "            \n",
    "            # Intance the grid search and choose Cross Validation folders \n",
    "            clf = GridSearchCV(\n",
    "                classifier, tuned_parameters, cv=10, scoring='%s' % score\n",
    "                )\n",
    "\n",
    "            X_train_vectorized = vect.fit_transform(X_train)\n",
    "            clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "            print(\"Best parameters set found:\")\n",
    "            print()\n",
    "            print(clf.best_params_)\n",
    "            print()\n",
    "            print(\"Best score result: \" + str(clf.best_score_))\n",
    "            print()\n",
    "            print(\"Scores on development set:\")\n",
    "            print()\n",
    "            \n",
    "            means = clf.cv_results_['mean_test_score']\n",
    "            stds = clf.cv_results_['std_test_score']\n",
    "            for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "                print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                      % (mean, std * 2, params))\n",
    "            print()\n",
    "            X_test_vectorized = vect.transform(X_test)\n",
    "            y_true, y_pred = y_test, clf.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters with CountVectorizer for accuracy\n",
      "\n",
      "Best parameters set found:\n",
      "\n",
      "{'loss': 'hinge', 'max_iter': 1000}\n",
      "\n",
      "Best score result: 0.8413105413105413\n",
      "\n",
      "Scores on development set:\n",
      "\n",
      "0.830 (+/-0.087) for {'loss': 'hinge', 'penalty': 'l1'}\n",
      "0.804 (+/-0.125) for {'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.811 (+/-0.118) for {'loss': 'squared_hinge', 'penalty': 'l1'}\n",
      "0.796 (+/-0.120) for {'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.834 (+/-0.113) for {'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "0.830 (+/-0.102) for {'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "0.841 (+/-0.125) for {'loss': 'hinge', 'max_iter': 1000}\n",
      "0.822 (+/-0.097) for {'loss': 'hinge', 'max_iter': 500}\n",
      "0.826 (+/-0.131) for {'loss': 'hinge', 'max_iter': 700}\n",
      "0.808 (+/-0.140) for {'loss': 'squared_hinge', 'max_iter': 1000}\n",
      "0.815 (+/-0.162) for {'loss': 'squared_hinge', 'max_iter': 500}\n",
      "0.796 (+/-0.122) for {'loss': 'squared_hinge', 'max_iter': 700}\n",
      "0.799 (+/-0.147) for {'loss': 'modified_huber', 'max_iter': 1000}\n",
      "0.834 (+/-0.113) for {'loss': 'modified_huber', 'max_iter': 500}\n",
      "0.823 (+/-0.126) for {'loss': 'modified_huber', 'max_iter': 700}\n",
      "\n",
      "# Tuning hyper-parameters with TfidfVectorizer for accuracy\n",
      "\n",
      "Best parameters set found:\n",
      "\n",
      "{'loss': 'hinge', 'max_iter': 1000}\n",
      "\n",
      "Best score result: 0.856980056980057\n",
      "\n",
      "Scores on development set:\n",
      "\n",
      "0.845 (+/-0.119) for {'loss': 'hinge', 'penalty': 'l1'}\n",
      "0.845 (+/-0.141) for {'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.781 (+/-0.132) for {'loss': 'squared_hinge', 'penalty': 'l1'}\n",
      "0.766 (+/-0.155) for {'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.841 (+/-0.149) for {'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "0.834 (+/-0.122) for {'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "0.857 (+/-0.108) for {'loss': 'hinge', 'max_iter': 1000}\n",
      "0.830 (+/-0.096) for {'loss': 'hinge', 'max_iter': 500}\n",
      "0.853 (+/-0.118) for {'loss': 'hinge', 'max_iter': 700}\n",
      "0.777 (+/-0.134) for {'loss': 'squared_hinge', 'max_iter': 1000}\n",
      "0.800 (+/-0.133) for {'loss': 'squared_hinge', 'max_iter': 500}\n",
      "0.777 (+/-0.139) for {'loss': 'squared_hinge', 'max_iter': 700}\n",
      "0.823 (+/-0.106) for {'loss': 'modified_huber', 'max_iter': 1000}\n",
      "0.822 (+/-0.090) for {'loss': 'modified_huber', 'max_iter': 500}\n",
      "0.830 (+/-0.132) for {'loss': 'modified_huber', 'max_iter': 700}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Set the parameters for the grid search\n",
    "tuned_parameters = [{'loss': ['hinge', 'squared_hinge', 'modified_huber'], 'penalty': ['l1', 'l2']},\n",
    "                    {'loss': ['hinge', 'squared_hinge', 'modified_huber'], 'max_iter': [1000, 500, 700]}]\n",
    "\n",
    "# Define the metrics to evaluate the model\n",
    "scores = ['accuracy']\n",
    "\n",
    "search_best_model(SGDClassifier(), scores, tuned_parameters, vectorizer_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters with CountVectorizer for accuracy\n",
      "\n",
      "Best parameters set found:\n",
      "\n",
      "{'C': 1.5, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "\n",
      "Best score result: 0.8756410256410257\n",
      "\n",
      "Scores on development set:\n",
      "\n",
      "0.777 (+/-0.144) for {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.857 (+/-0.111) for {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.864 (+/-0.114) for {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.864 (+/-0.114) for {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "0.706 (+/-0.128) for {'C': 0.5, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 0.5, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 0.5, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.823 (+/-0.156) for {'C': 0.5, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.838 (+/-0.126) for {'C': 0.5, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.838 (+/-0.126) for {'C': 0.5, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "nan (+/-nan) for {'C': 0.5, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 0.5, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 0.5, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "0.819 (+/-0.102) for {'C': 1.5, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 1.5, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 1.5, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.876 (+/-0.082) for {'C': 1.5, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.872 (+/-0.108) for {'C': 1.5, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.872 (+/-0.108) for {'C': 1.5, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "nan (+/-nan) for {'C': 1.5, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 1.5, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 1.5, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "\n",
      "# Tuning hyper-parameters with TfidfVectorizer for accuracy\n",
      "\n",
      "Best parameters set found:\n",
      "\n",
      "{'C': 1.5, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      "Best score result: 0.8641025641025643\n",
      "\n",
      "Scores on development set:\n",
      "\n",
      "0.747 (+/-0.160) for {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.815 (+/-0.075) for {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.822 (+/-0.062) for {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.822 (+/-0.062) for {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "0.626 (+/-0.151) for {'C': 0.5, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 0.5, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 0.5, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.739 (+/-0.106) for {'C': 0.5, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.747 (+/-0.109) for {'C': 0.5, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.747 (+/-0.109) for {'C': 0.5, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "nan (+/-nan) for {'C': 0.5, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 0.5, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 0.5, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "0.788 (+/-0.173) for {'C': 1.5, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 1.5, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 1.5, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.857 (+/-0.066) for {'C': 1.5, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.864 (+/-0.069) for {'C': 1.5, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.864 (+/-0.069) for {'C': 1.5, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "nan (+/-nan) for {'C': 1.5, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 1.5, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'C': 1.5, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set the parameters for the grid search\n",
    "tuned_parameters = [{'penalty': ['l1', 'l2', 'elasticnet'], 'C': [1, 0.5, 1.5],\n",
    "                    'solver': ['liblinear', 'lbfgs', 'newton-cg']}]\n",
    "\n",
    "# Define the metrics to evaluate the model\n",
    "scores = ['accuracy']\n",
    "\n",
    "search_best_model(LogisticRegression(), scores, tuned_parameters, vectorizer_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters with CountVectorizer for accuracy\n",
      "\n",
      "Best parameters set found:\n",
      "\n",
      "{'C': 0.7, 'kernel': 'linear'}\n",
      "\n",
      "Best score result: 0.8717948717948717\n",
      "\n",
      "Scores on development set:\n",
      "\n",
      "0.827 (+/-0.156) for {'C': 0.5, 'kernel': 'linear'}\n",
      "0.630 (+/-0.188) for {'C': 0.5, 'kernel': 'poly'}\n",
      "0.676 (+/-0.155) for {'C': 0.5, 'kernel': 'rbf'}\n",
      "0.759 (+/-0.129) for {'C': 0.5, 'kernel': 'sigmoid'}\n",
      "0.872 (+/-0.141) for {'C': 0.7, 'kernel': 'linear'}\n",
      "0.657 (+/-0.168) for {'C': 0.7, 'kernel': 'poly'}\n",
      "0.755 (+/-0.176) for {'C': 0.7, 'kernel': 'rbf'}\n",
      "0.830 (+/-0.134) for {'C': 0.7, 'kernel': 'sigmoid'}\n",
      "0.864 (+/-0.141) for {'C': 1, 'kernel': 'linear'}\n",
      "0.687 (+/-0.150) for {'C': 1, 'kernel': 'poly'}\n",
      "0.819 (+/-0.132) for {'C': 1, 'kernel': 'rbf'}\n",
      "0.838 (+/-0.138) for {'C': 1, 'kernel': 'sigmoid'}\n",
      "0.841 (+/-0.127) for {'C': 1.5, 'kernel': 'linear'}\n",
      "0.683 (+/-0.156) for {'C': 1.5, 'kernel': 'poly'}\n",
      "0.849 (+/-0.108) for {'C': 1.5, 'kernel': 'rbf'}\n",
      "0.861 (+/-0.156) for {'C': 1.5, 'kernel': 'sigmoid'}\n",
      "\n",
      "# Tuning hyper-parameters with TfidfVectorizer for accuracy\n",
      "\n",
      "Best parameters set found:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Best score result: 0.8831908831908832\n",
      "\n",
      "Scores on development set:\n",
      "\n",
      "0.830 (+/-0.115) for {'C': 0.5, 'kernel': 'linear'}\n",
      "0.551 (+/-0.159) for {'C': 0.5, 'kernel': 'poly'}\n",
      "0.672 (+/-0.140) for {'C': 0.5, 'kernel': 'rbf'}\n",
      "0.800 (+/-0.132) for {'C': 0.5, 'kernel': 'sigmoid'}\n",
      "0.876 (+/-0.117) for {'C': 0.7, 'kernel': 'linear'}\n",
      "0.664 (+/-0.133) for {'C': 0.7, 'kernel': 'poly'}\n",
      "0.784 (+/-0.095) for {'C': 0.7, 'kernel': 'rbf'}\n",
      "0.861 (+/-0.096) for {'C': 0.7, 'kernel': 'sigmoid'}\n",
      "0.883 (+/-0.133) for {'C': 1, 'kernel': 'linear'}\n",
      "0.720 (+/-0.143) for {'C': 1, 'kernel': 'poly'}\n",
      "0.841 (+/-0.119) for {'C': 1, 'kernel': 'rbf'}\n",
      "0.880 (+/-0.142) for {'C': 1, 'kernel': 'sigmoid'}\n",
      "0.879 (+/-0.143) for {'C': 1.5, 'kernel': 'linear'}\n",
      "0.728 (+/-0.153) for {'C': 1.5, 'kernel': 'poly'}\n",
      "0.860 (+/-0.103) for {'C': 1.5, 'kernel': 'rbf'}\n",
      "0.880 (+/-0.150) for {'C': 1.5, 'kernel': 'sigmoid'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Set the parameters for the grid search\n",
    "tuned_parameters = [{'C': [0.5, 0.7, 1, 1.5], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}]\n",
    "\n",
    "# Define the metrics to evaluate the model\n",
    "scores = ['accuracy']\n",
    "\n",
    "search_best_model(SVC(), scores, tuned_parameters, vectorizer_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods - Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters with CountVectorizer for accuracy\n",
      "\n",
      "Best parameters set found:\n",
      "\n",
      "{'max_depth': 10, 'n_estimators': 300}\n",
      "\n",
      "Best score result: 0.7659544159544159\n",
      "\n",
      "Scores on development set:\n",
      "\n",
      "0.566 (+/-0.116) for {'max_depth': 4, 'n_estimators': 50}\n",
      "0.521 (+/-0.151) for {'max_depth': 4, 'n_estimators': 70}\n",
      "0.562 (+/-0.223) for {'max_depth': 4, 'n_estimators': 100}\n",
      "0.532 (+/-0.134) for {'max_depth': 4, 'n_estimators': 150}\n",
      "0.532 (+/-0.113) for {'max_depth': 4, 'n_estimators': 300}\n",
      "0.694 (+/-0.152) for {'max_depth': 7, 'n_estimators': 50}\n",
      "0.710 (+/-0.192) for {'max_depth': 7, 'n_estimators': 70}\n",
      "0.709 (+/-0.188) for {'max_depth': 7, 'n_estimators': 100}\n",
      "0.683 (+/-0.211) for {'max_depth': 7, 'n_estimators': 150}\n",
      "0.709 (+/-0.178) for {'max_depth': 7, 'n_estimators': 300}\n",
      "0.758 (+/-0.134) for {'max_depth': 10, 'n_estimators': 50}\n",
      "0.758 (+/-0.180) for {'max_depth': 10, 'n_estimators': 70}\n",
      "0.755 (+/-0.179) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.762 (+/-0.153) for {'max_depth': 10, 'n_estimators': 150}\n",
      "0.766 (+/-0.152) for {'max_depth': 10, 'n_estimators': 300}\n",
      "\n",
      "# Tuning hyper-parameters with TfidfVectorizer for accuracy\n",
      "\n",
      "Best parameters set found:\n",
      "\n",
      "{'max_depth': 10, 'n_estimators': 300}\n",
      "\n",
      "Best score result: 0.7621082621082621\n",
      "\n",
      "Scores on development set:\n",
      "\n",
      "0.566 (+/-0.104) for {'max_depth': 4, 'n_estimators': 50}\n",
      "0.543 (+/-0.104) for {'max_depth': 4, 'n_estimators': 70}\n",
      "0.585 (+/-0.171) for {'max_depth': 4, 'n_estimators': 100}\n",
      "0.563 (+/-0.133) for {'max_depth': 4, 'n_estimators': 150}\n",
      "0.554 (+/-0.159) for {'max_depth': 4, 'n_estimators': 300}\n",
      "0.713 (+/-0.138) for {'max_depth': 7, 'n_estimators': 50}\n",
      "0.683 (+/-0.181) for {'max_depth': 7, 'n_estimators': 70}\n",
      "0.687 (+/-0.211) for {'max_depth': 7, 'n_estimators': 100}\n",
      "0.698 (+/-0.182) for {'max_depth': 7, 'n_estimators': 150}\n",
      "0.683 (+/-0.161) for {'max_depth': 7, 'n_estimators': 300}\n",
      "0.728 (+/-0.155) for {'max_depth': 10, 'n_estimators': 50}\n",
      "0.755 (+/-0.152) for {'max_depth': 10, 'n_estimators': 70}\n",
      "0.755 (+/-0.165) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.755 (+/-0.165) for {'max_depth': 10, 'n_estimators': 150}\n",
      "0.762 (+/-0.158) for {'max_depth': 10, 'n_estimators': 300}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set the parameters for the grid search\n",
    "tuned_parameters = [{'n_estimators': [50, 70, 100, 150, 300],\n",
    "                     'max_depth': [4, 7, 10]}]\n",
    "\n",
    "# Define the metrics to evaluate the model\n",
    "scores = ['accuracy']\n",
    "\n",
    "search_best_model(RandomForestClassifier(), scores, tuned_parameters, vectorizer_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realized that using ensemble models the efficiency goes down and time processing is higher. It's really nothing appropriated for this job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Bernoulli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters with CountVectorizer for accuracy\n",
      "\n",
      "Best parameters set found:\n",
      "\n",
      "{'alpha': 0.3, 'fit_prior': True}\n",
      "\n",
      "Best score result: 0.8679487179487181\n",
      "\n",
      "Scores on development set:\n",
      "\n",
      "0.868 (+/-0.118) for {'alpha': 0.3, 'fit_prior': True}\n",
      "0.864 (+/-0.112) for {'alpha': 0.3, 'fit_prior': False}\n",
      "0.841 (+/-0.102) for {'alpha': 0.5, 'fit_prior': True}\n",
      "0.853 (+/-0.132) for {'alpha': 0.5, 'fit_prior': False}\n",
      "0.702 (+/-0.137) for {'alpha': 1.0, 'fit_prior': True}\n",
      "0.759 (+/-0.119) for {'alpha': 1.0, 'fit_prior': False}\n",
      "\n",
      "# Tuning hyper-parameters with TfidfVectorizer for accuracy\n",
      "\n",
      "Best parameters set found:\n",
      "\n",
      "{'alpha': 0.3, 'fit_prior': True}\n",
      "\n",
      "Best score result: 0.8679487179487181\n",
      "\n",
      "Scores on development set:\n",
      "\n",
      "0.868 (+/-0.118) for {'alpha': 0.3, 'fit_prior': True}\n",
      "0.864 (+/-0.112) for {'alpha': 0.3, 'fit_prior': False}\n",
      "0.841 (+/-0.102) for {'alpha': 0.5, 'fit_prior': True}\n",
      "0.853 (+/-0.132) for {'alpha': 0.5, 'fit_prior': False}\n",
      "0.702 (+/-0.137) for {'alpha': 1.0, 'fit_prior': True}\n",
      "0.759 (+/-0.119) for {'alpha': 1.0, 'fit_prior': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Set the parameters for the grid search\n",
    "tuned_parameters = [{'alpha': [0.3, 0.5, 1.0],\n",
    "                     'fit_prior': [True, False]}]\n",
    "\n",
    "# Define the metrics to evaluate the model\n",
    "scores = ['accuracy']\n",
    "\n",
    "search_best_model(BernoulliNB(), scores, tuned_parameters, vectorizer_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing our model evaluation pipeline we arrive at the best result for accuracy:\n",
    "\n",
    "- **Best Vectorizer Technique:** Tfidf Vectorizer\n",
    "- **Best Model:** Support Vector Classification (SVC)\n",
    "- **Best parameters set found:** {'C': 1, 'kernel': 'linear'}\n",
    "\n",
    "**Best score result: 0.8831908831908832**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the resulting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining the model with full dataset to do the interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# best_params = {'loss': 'hinge', 'max_iter': 1000}\n",
    "\n",
    "clf = SVC(C=1, kernel='linear', probability=True)\n",
    "# Shuffling all dataframe to train\n",
    "df_train = shuffle(df)\n",
    "\n",
    "X = df_train['phrase']\n",
    "y = df_train['intent']\n",
    "\n",
    "clf.fit(tfid_vectorizer.fit_transform(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes to be predicted:\n",
      "['competencias' 'definicoes' 'digitalizacao' 'documentacao'\n",
      " 'processos_administrativos' 'usuario']\n",
      "\n",
      "Prediction Confidence:\n",
      "[0.01228124 0.01699306 0.01554672 0.01881749 0.89581218 0.04054931]\n",
      "\n",
      "Intent Prediction:\n",
      "processos_administrativos\n"
     ]
    }
   ],
   "source": [
    "# TYPE YOUR INPUT PHRASE HERE:\n",
    "INPUT_PHRASE = 'como ocorre a transferência de processos para o SEI?'\n",
    "\n",
    "\n",
    "# Modeling phrase to model predict:\n",
    "input_dict = {'phrase': [INPUT_PHRASE]}\n",
    "df_input = pd.DataFrame.from_dict(input_dict)\n",
    "\n",
    "\n",
    "print(\"Classes to be predicted:\")\n",
    "print(clf.classes_, end='\\n\\n')\n",
    "print(\"Prediction Confidence:\", end='\\n')\n",
    "print(clf.predict_proba(tfid_vectorizer.transform(df_input['phrase']))[0], end='\\n\\n')\n",
    "print(\"Intent Prediction:\")\n",
    "print(clf.predict(tfid_vectorizer.transform(df_input['phrase']))[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlp-test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
